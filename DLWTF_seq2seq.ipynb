{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 2734496,
          "sourceType": "datasetVersion",
          "datasetId": 1654566
        }
      ],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "DLWTF seq2seq",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SatChittAnand/Seq2Seq-model-newspaper-text-summarization/blob/main/DLWTF_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "gowrishankarp_newspaper_text_summarization_cnn_dailymail_path = kagglehub.dataset_download('gowrishankarp/newspaper-text-summarization-cnn-dailymail')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "5dCtFrtPSxdJ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T06:53:12.695976Z",
          "iopub.execute_input": "2025-12-21T06:53:12.696719Z",
          "iopub.status.idle": "2025-12-21T06:53:12.961355Z",
          "shell.execute_reply.started": "2025-12-21T06:53:12.696693Z",
          "shell.execute_reply": "2025-12-21T06:53:12.96058Z"
        },
        "id": "NHtEi6F0SxdL",
        "outputId": "0768202c-1c13-4a24-bc03-f85f1305216c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv\n/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\n/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train, validation, and test splits\n",
        "train_df = pd.read_csv(\"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\")\n",
        "val_df   = pd.read_csv(\"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv\")\n",
        "test_df  = pd.read_csv(\"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv\")\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Validation shape:\", val_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)\n",
        "\n",
        "# Peek at the first few rows\n",
        "train_df.head()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T06:53:17.431063Z",
          "iopub.execute_input": "2025-12-21T06:53:17.431506Z",
          "iopub.status.idle": "2025-12-21T06:53:37.751523Z",
          "shell.execute_reply.started": "2025-12-21T06:53:17.431481Z",
          "shell.execute_reply": "2025-12-21T06:53:37.75076Z"
        },
        "id": "Qfw-wZ74SxdM",
        "outputId": "4e27cda1-43ca-48e9-a1b7-322d87dc3dbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Train shape: (287113, 3)\nValidation shape: (13368, 3)\nTest shape: (11490, 3)\n",
          "output_type": "stream"
        },
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                         id  \\\n0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n2  00027e965c8264c35cc1bc55556db388da82b07f   \n3  0002c17436637c4fe1837c935c04de47adb18e9a   \n4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n\n                                             article  \\\n0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n1  (CNN) -- Ralph Mata was an internal affairs li...   \n2  A drunk driver who killed a young woman in a h...   \n3  (CNN) -- With a breezy sweep of his pen Presid...   \n4  Fleetwood are the only team still to have a 10...   \n\n                                          highlights  \n0  Bishop John Folda, of North Dakota, is taking ...  \n1  Criminal complaint: Cop used his role to help ...  \n2  Craig Eccleston-Todd, 27, had drunk at least t...  \n3  Nina dos Santos says Europe must be ready to a...  \n4  Fleetwood top of League One after 2-0 win at S...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>article</th>\n      <th>highlights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n      <td>Criminal complaint: Cop used his role to help ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n      <td>A drunk driver who killed a young woman in a h...</td>\n      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n      <td>Nina dos Santos says Europe must be ready to a...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n      <td>Fleetwood are the only team still to have a 10...</td>\n      <td>Fleetwood top of League One after 2-0 win at S...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.columns)\n",
        "print(train_df.iloc[0][\"article\"][:500])   # preview first 500 chars\n",
        "print(train_df.iloc[0][\"highlights\"])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T06:53:44.631141Z",
          "iopub.execute_input": "2025-12-21T06:53:44.631419Z",
          "iopub.status.idle": "2025-12-21T06:53:44.638848Z",
          "shell.execute_reply.started": "2025-12-21T06:53:44.631395Z",
          "shell.execute_reply": "2025-12-21T06:53:44.638057Z"
        },
        "id": "4q38ZvhaSxdN",
        "outputId": "34b165be-f38a-4865-81f3-1127d9023853"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Index(['id', 'article', 'highlights'], dtype='object')\nBy . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in N\nBishop John Folda, of North Dakota, is taking time off after being diagnosed .\nHe contracted the infection through contaminated food in Italy .\nChurch members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "# Example encoding\n",
        "sample = tokenizer(train_df.iloc[0][\"article\"], max_length=512, truncation=True, padding=\"max_length\")\n",
        "summary = tokenizer(train_df.iloc[0][\"highlights\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "print(sample[\"input_ids\"][:20])\n",
        "print(summary[\"input_ids\"][:20])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T06:53:48.256008Z",
          "iopub.execute_input": "2025-12-21T06:53:48.256703Z",
          "iopub.status.idle": "2025-12-21T06:53:59.547602Z",
          "shell.execute_reply.started": "2025-12-21T06:53:48.256675Z",
          "shell.execute_reply": "2025-12-21T06:53:59.546898Z"
        },
        "id": "hmlc6G4NSxdN",
        "outputId": "22fe4d83-06ff-407c-b835-bbf210ae15bd",
        "colab": {
          "referenced_widgets": [
            "df5bcc00f3d743f1bd43f3c849441fa4",
            "34e70f77a42f4f50a5a6af5a5123c468",
            "ee0cb039f9c647258fd2d24b06d99f4e",
            "610665de67f44469a417a3589904e6a6"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df5bcc00f3d743f1bd43f3c849441fa4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34e70f77a42f4f50a5a6af5a5123c468"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee0cb039f9c647258fd2d24b06d99f4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "610665de67f44469a417a3589904e6a6"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[0, 2765, 479, 1562, 977, 479, 29731, 7976, 14849, 1691, 35, 479, 501, 35, 1225, 12936, 6, 564, 779, 1014]\n[0, 387, 44517, 610, 41303, 102, 6, 9, 369, 6223, 6, 16, 602, 86, 160, 71, 145, 6443, 479, 50118]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# class CNNDailyMailDataset(Dataset):\n",
        "#     def __init__(self, df, tokenizer, max_input_len=512, max_output_len=128):\n",
        "#         self.df = df\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_input_len = max_input_len\n",
        "#         self.max_output_len = max_output_len\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         article = self.df.iloc[idx][\"article\"]\n",
        "#         summary = self.df.iloc[idx][\"highlights\"]\n",
        "#         enc = self.tokenizer(article, max_length=self.max_import torch)\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# class CNNDailyMailDataset(Dataset):\n",
        "#     def __init__(self, df, tokenizer, max_input_len=512, max_output_len=128):\n",
        "#         self.df = df\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_input_len = max_input_len\n",
        "#         self.max_output_len = max_output_len\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         article = self.df.iloc[idx][\"article\"]\n",
        "#         summary = self.df.iloc[idx][\"highlights\"]\n",
        "#         enc = self.tokenizer(article, max_length=self.max_input_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "#         dec = self.tokenizer(summary, max_length=self.max_output_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "#         return {\n",
        "#             \"src\": enc[\"input_ids\"].squeeze(),\n",
        "#             \"src_mask\": enc[\"attention_mask\"].squeeze(),\n",
        "#             \"tgt\": dec[\"input_ids\"].squeeze(),\n",
        "#             \"tgt_mask\": dec[\"attention_mask\"].squeeze()\n",
        "#         }\n",
        "\n",
        "# train_dataset = CNNDailyMatilDataset(train_df, tokenizer)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "# input_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "#         dec = self.tokenizer(summary, max_length=self.max_output_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "#         return {\n",
        "#             \"src\": enc[\"input_ids\"].squeeze(),\n",
        "#             \"src_mask\": enc[\"attention_mask\"].squeeze(),\n",
        "#             \"tgt\": dec[\"input_ids\"].squeeze(),\n",
        "#             \"tgt_mask\": dec[\"attention_mask\"].squeeze()\n",
        "#         }\n",
        "\n",
        "# train_dataset = CNNDailyMatilDataset(train_df, tokenizer)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CNNDailyMailDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_input_len=512, max_output_len=128):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_len = max_input_len\n",
        "        self.max_output_len = max_output_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.df.iloc[idx][\"article\"]\n",
        "        summary = self.df.iloc[idx][\"highlights\"]\n",
        "\n",
        "        # Encode article\n",
        "        enc = self.tokenizer(\n",
        "            article,\n",
        "            max_length=self.max_input_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Encode summary\n",
        "        dec = self.tokenizer(\n",
        "            summary,\n",
        "            max_length=self.max_output_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"src\": enc[\"input_ids\"].squeeze(),\n",
        "            \"src_mask\": enc[\"attention_mask\"].squeeze(),\n",
        "            \"tgt\": dec[\"input_ids\"].squeeze(),\n",
        "            \"tgt_mask\": dec[\"attention_mask\"].squeeze()\n",
        "        }\n",
        "\n",
        "# âœ… Correct dataset instantiation\n",
        "train_dataset = CNNDailyMailDataset(train_df, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T08:18:04.362303Z",
          "iopub.execute_input": "2025-12-21T08:18:04.363281Z",
          "iopub.status.idle": "2025-12-21T08:18:04.37358Z",
          "shell.execute_reply.started": "2025-12-21T08:18:04.363253Z",
          "shell.execute_reply": "2025-12-21T08:18:04.372891Z"
        },
        "id": "BC7seeMWSxdO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, enc_dim, dec_dim, attn_dim):\n",
        "        super().__init__()\n",
        "        self.W_h = nn.Linear(enc_dim, attn_dim, bias=False)\n",
        "        self.W_s = nn.Linear(dec_dim, attn_dim, bias=True)\n",
        "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, dec_state, enc_outputs, mask=None):\n",
        "        # enc_outputs: [b, T, enc_dim], dec_state: [b, dec_dim]\n",
        "        Wh = self.W_h(enc_outputs)                      # [b, T, attn_dim]\n",
        "        Ws = self.W_s(dec_state).unsqueeze(1)          # [b, 1, attn_dim]\n",
        "        scores = self.v(torch.tanh(Wh + Ws)).squeeze(-1)  # [b, T]\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(~mask, -1e9)\n",
        "        attn_weights = F.softmax(scores, dim=-1)        # [b, T]\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), enc_outputs).squeeze(1)  # [b, enc_dim]\n",
        "        return context, attn_weights\n",
        "\n",
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, enc_dim, dec_dim, general=True):\n",
        "        super().__init__()\n",
        "        self.general = general\n",
        "        self.W = nn.Linear(enc_dim, dec_dim, bias=False) if general else None\n",
        "\n",
        "    def forward(self, dec_state, enc_outputs, mask=None):\n",
        "        # dec_state: [b, dec_dim], enc_outputs: [b, T, enc_dim]\n",
        "        if self.general:\n",
        "            proj = self.W(enc_outputs)                  # [b, T, dec_dim]\n",
        "            scores = torch.bmm(proj, dec_state.unsqueeze(-1)).squeeze(-1)  # [b, T]\n",
        "        else:\n",
        "            # dot\n",
        "            scores = torch.bmm(enc_outputs, dec_state.unsqueeze(-1)).squeeze(-1)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(~mask, -1e9)\n",
        "        attn_weights = F.softmax(scores, dim=-1)        # [b, T]\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), enc_outputs).squeeze(1)  # [b, enc_dim]\n",
        "        return context, attn_weights\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out_dim = 2 * hid_dim\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        emb = self.dropout(self.emb(x))\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        out_packed, (h, c) = self.rnn(packed)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)\n",
        "        # concat final states from both directions\n",
        "        h_cat = torch.cat([h[-2], h[-1]], dim=-1)\n",
        "        c_cat = torch.cat([c[-2], c[-1]], dim=-1)\n",
        "        return outputs, (h_cat, c_cat)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, enc_dim, dec_hid, attention, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(emb_dim + enc_dim, dec_hid, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.attention = attention\n",
        "        self.out = nn.Linear(dec_hid + enc_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, y_prev, dec_state, enc_outputs, enc_mask):\n",
        "        emb = self.dropout(self.emb(y_prev))  # [b, 1, emb_dim]\n",
        "        h_t = dec_state[0][-1]                # top layer hidden: [b, dec_hid]\n",
        "        context, attn = self.attention(h_t, enc_outputs, mask=enc_mask)\n",
        "        rnn_in = torch.cat([emb, context.unsqueeze(1)], dim=-1)\n",
        "        out, dec_state = self.rnn(rnn_in, dec_state)\n",
        "        out = out.squeeze(1)                  # [b, dec_hid]\n",
        "        logits = self.out(torch.cat([out, context], dim=-1))\n",
        "        return logits, dec_state, attn\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, bos_id, eos_id, max_len=160):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.bos_id = bos_id\n",
        "        self.eos_id = eos_id\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, src, src_lengths, tgt=None, teacher_forcing=0.5):\n",
        "        enc_outputs, (h0, c0) = self.encoder(src, src_lengths)\n",
        "        # bridge to decoder hidden size if dimensions differ\n",
        "        dec_hid = self.decoder.rnn.hidden_size\n",
        "        if h0.shape[-1] != dec_hid:\n",
        "            bridge_h = nn.Linear(h0.shape[-1], dec_hid, bias=False).to(h0.device)\n",
        "            bridge_c = nn.Linear(c0.shape[-1], dec_hid, bias=False).to(c0.device)\n",
        "            h0 = bridge_h(h0).unsqueeze(0)\n",
        "            c0 = bridge_c(c0).unsqueeze(0)\n",
        "        else:\n",
        "            h0, c0 = h0.unsqueeze(0), c0.unsqueeze(0)\n",
        "\n",
        "        dec_state = (h0, c0)\n",
        "        batch_size = src.size(0)\n",
        "        enc_mask = src.ne(0)  # [b, T]\n",
        "        outputs = []\n",
        "        y = torch.full((batch_size, 1), self.bos_id, dtype=torch.long, device=src.device)\n",
        "\n",
        "        if tgt is not None:\n",
        "            T = tgt.size(1)\n",
        "            for t in range(T - 1):\n",
        "                logits, dec_state, _ = self.decoder(y, dec_state, enc_outputs, enc_mask)\n",
        "                outputs.append(logits.unsqueeze(1))\n",
        "                teacher = torch.rand(1).item() < teacher_forcing\n",
        "                y = (tgt[:, t+1:t+2] if teacher else logits.argmax(-1, keepdim=True))\n",
        "            return torch.cat(outputs, dim=1)  # [b, T-1, vocab]\n",
        "        else:\n",
        "            # greedy decode\n",
        "            hyps = []\n",
        "            for t in range(self.max_len):\n",
        "                logits, dec_state, _ = self.decoder(y, dec_state, enc_outputs, enc_mask)\n",
        "                y = logits.argmax(-1, keepdim=True)\n",
        "                hyps.append(y)\n",
        "                if torch.all(y.squeeze(1) == self.eos_id):\n",
        "                    break\n",
        "            return torch.cat(hyps, dim=1)  # [b, <=max_len]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T08:18:06.987485Z",
          "iopub.execute_input": "2025-12-21T08:18:06.988084Z",
          "iopub.status.idle": "2025-12-21T08:18:07.006416Z",
          "shell.execute_reply.started": "2025-12-21T08:18:06.988059Z",
          "shell.execute_reply": "2025-12-21T08:18:07.005721Z"
        },
        "id": "I118JlESSxdP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.device_count())   # should print 2\n",
        "print(torch.cuda.get_device_name(0))\n",
        "print(torch.cuda.get_device_name(1))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T08:18:09.221951Z",
          "iopub.execute_input": "2025-12-21T08:18:09.222471Z",
          "iopub.status.idle": "2025-12-21T08:18:09.226501Z",
          "shell.execute_reply.started": "2025-12-21T08:18:09.222446Z",
          "shell.execute_reply": "2025-12-21T08:18:09.225879Z"
        },
        "id": "1dPoy4ScSxdQ",
        "outputId": "ef567308-4b06-4028-a96f-5b934e7e10be"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "2\nTesla T4\nTesla T4\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "PAD = tokenizer.pad_token_id\n",
        "BOS = tokenizer.bos_token_id or tokenizer.cls_token_id\n",
        "EOS = tokenizer.eos_token_id or tokenizer.sep_token_id\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T08:18:16.702595Z",
          "iopub.execute_input": "2025-12-21T08:18:16.703244Z",
          "iopub.status.idle": "2025-12-21T08:18:16.707858Z",
          "shell.execute_reply.started": "2025-12-21T08:18:16.703215Z",
          "shell.execute_reply": "2025-12-21T08:18:16.706991Z"
        },
        "id": "7-705aYcSxdQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "vocab_size = tokenizer.vocab_size\n",
        "emb_dim = 256\n",
        "hidden_size = 512\n",
        "\n",
        "# Choose attention type\n",
        "attention = BahdanauAttention(enc_dim=hidden_size*2, dec_dim=hidden_size, attn_dim=hidden_size)\n",
        "# Or: attention = LuongAttention(enc_dim=hidden_size*2, dec_dim=hidden_size, general=True)\n",
        "\n",
        "# Build encoder and decoder\n",
        "encoder = Encoder(vocab_size, emb_dim, hidden_size)\n",
        "decoder = Decoder(vocab_size, emb_dim, encoder.out_dim, hidden_size, attention)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T08:18:20.552003Z",
          "iopub.execute_input": "2025-12-21T08:18:20.552268Z",
          "iopub.status.idle": "2025-12-21T08:18:21.438966Z",
          "shell.execute_reply.started": "2025-12-21T08:18:20.552247Z",
          "shell.execute_reply": "2025-12-21T08:18:21.43813Z"
        },
        "id": "wrRmwUY_SxdR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = Seq2Seq(encoder, decoder, bos_id=BOS, eos_id=EOS).to(device)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T08:18:23.857486Z",
          "iopub.execute_input": "2025-12-21T08:18:23.858059Z",
          "iopub.status.idle": "2025-12-21T08:18:23.98653Z",
          "shell.execute_reply.started": "2025-12-21T08:18:23.858034Z",
          "shell.execute_reply": "2025-12-21T08:18:23.985744Z"
        },
        "id": "IUopCVOZSxdR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD, label_smoothing=0.1)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T08:18:28.53711Z",
          "iopub.execute_input": "2025-12-21T08:18:28.537368Z",
          "iopub.status.idle": "2025-12-21T08:18:28.542475Z",
          "shell.execute_reply.started": "2025-12-21T08:18:28.537348Z",
          "shell.execute_reply": "2025-12-21T08:18:28.541817Z"
        },
        "id": "jMSbKqVCSxdR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2  # adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        src = batch[\"src\"].to(device)\n",
        "        tgt = batch[\"tgt\"].to(device)\n",
        "        src_len = batch[\"src_mask\"].sum(dim=1)  # lengths from mask\n",
        "\n",
        "        logits = model(src, src_len, tgt, teacher_forcing=0.5)\n",
        "        target = tgt[:, 1:].contiguous()\n",
        "\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T08:18:37.268428Z",
          "iopub.execute_input": "2025-12-21T08:18:37.26927Z"
        },
        "id": "q-6sPTpNSxdR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "XdVDpgxtSxdS"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}