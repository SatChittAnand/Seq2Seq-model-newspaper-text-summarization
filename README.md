## Seq2Seq Model Newspaper Text Summarization

---

## ðŸ“‚ Data Loading
- **`import numpy as np`** â†’ Loads NumPy, a library for numerical computations.
- **`import pandas as pd`** â†’ Loads Pandas, used for handling tabular data (CSV files).
- **`os.walk('/kaggle/input')`** â†’ Iterates through files in the Kaggle input directory.
- **`pd.read_csv(...)`** â†’ Reads CSV files into Pandas DataFrames (`train_df`, `val_df`, `test_df`).

---

## ðŸ“° Dataset Structure
- **`train_df.shape`** â†’ Shows rows Ã— columns of the training dataset.
- **`train_df.head()`** â†’ Displays the first few rows.
- **`train_df.columns`** â†’ Lists column names (e.g., `"article"`, `"highlights"`).
- **`train_df.iloc[0]["article"]`** â†’ Accesses the first article.
- **`train_df.iloc[0]["highlights"]`** â†’ Accesses the summary (target).

---

## ðŸ”¤ Tokenization
- **`AutoTokenizer.from_pretrained("facebook/bart-base")`** â†’ Loads a pretrained tokenizer for BART.
- **`tokenizer(..., max_length=512, truncation=True, padding="max_length")`**  
  - `max_length` â†’ Maximum sequence length.  
  - `truncation=True` â†’ Cuts off text longer than `max_length`.  
  - `padding="max_length"` â†’ Pads shorter sequences to fixed length.  
- **`input_ids`** â†’ Token IDs (numerical representation of text).  
- **`attention_mask`** â†’ Marks which tokens are real vs. padding.

---

##  Dataset Class
- **`torch.utils.data.Dataset`** â†’ Base class for custom datasets.
- **`__len__`** â†’ Returns dataset size.
- **`__getitem__`** â†’ Defines how to fetch one sample (article + summary).
- **`DataLoader(..., batch_size=16, shuffle=True)`** â†’ Loads batches of 16 samples, shuffling order.

---

##  Attention Mechanisms
### Bahdanau Attention
- **Additive attention**: Combines encoder outputs + decoder state with learned weights.
- **`self.W_h` / `self.W_s`** â†’ Linear transformations of encoder/decoder states.
- **`torch.tanh(...)`** â†’ Non-linear activation.
- **`F.softmax(scores, dim=-1)`** â†’ Converts scores into probability distribution.
- **`context`** â†’ Weighted sum of encoder outputs.

### Luong Attention
- **Dot-product / general attention**: Uses similarity between encoder outputs and decoder state.
- **`self.W`** â†’ Optional linear projection for "general" attention.
- **`torch.bmm(...)`** â†’ Batch matrix multiplication for computing scores.

---

##  Encoder
- **`nn.Embedding(vocab_size, emb_dim)`** â†’ Converts token IDs into dense vectors.
- **`nn.LSTM(..., bidirectional=True)`** â†’ Processes sequence forward + backward.
- **`pack_padded_sequence`** â†’ Efficiently handles variable-length sequences.
- **`h_cat` / `c_cat`** â†’ Concatenated hidden and cell states from both directions.

---

##  Decoder
- **`nn.Embedding`** â†’ Embeds target tokens.
- **`self.rnn`** â†’ LSTM that takes both embeddings + context vector.
- **`self.attention`** â†’ Computes context from encoder outputs.
- **`self.out`** â†’ Linear layer projecting to vocabulary size (logits).
- **`logits`** â†’ Raw scores for each word in vocabulary.

---

##  Seq2Seq Model
- **`bos_id` / `eos_id`** â†’ Special tokens for beginning/end of sequence.
- **Teacher forcing** â†’ During training, sometimes feed the true next token instead of predicted.
- **Greedy decoding** â†’ At inference, pick the highest-probability token at each step.

---

##  Training Setup
- **`torch.cuda.device_count()`** â†’ Number of GPUs available.
- **`torch.device("cuda" if torch.cuda.is_available() else "cpu")`** â†’ Chooses GPU if available.
- **`PAD` / `BOS` / `EOS`** â†’ Special token IDs from tokenizer.
- **Hyperparameters**:  
  - `emb_dim = 256` â†’ Embedding size.  
  - `hidden_size = 512` â†’ LSTM hidden dimension.  
- **`optim.Adam(...)`** â†’ Optimizer for gradient descent.
- **`nn.CrossEntropyLoss(ignore_index=PAD, label_smoothing=0.1)`** â†’ Loss function ignoring padding tokens, smoothing labels.

---

##  Training Loop
- **`num_epochs = 2`** â†’ Number of passes through dataset.
- **`model.train()`** â†’ Sets model to training mode.
- **`src_len = batch["src_mask"].sum(dim=1)`** â†’ Computes actual sequence lengths.
- **`loss.backward()`** â†’ Backpropagation.
- **`clip_grad_norm_`** â†’ Prevents exploding gradients.
- **`optimizer.step()`** â†’ Updates weights.
- **`total_loss/len(train_loader)`** â†’ Average loss per epoch.

---

##  Multi-GPU Training
- **`nn.DataParallel(model)`** â†’ Wraps model to run on multiple GPUs.
- **`model.to(device)`** â†’ Moves model to GPU(s).

---

## Comparison Table

| Aspect | **Bahdanau Attention (Additive)** | **Luong Attention (Multiplicative)** |
|--------|-----------------------------------|--------------------------------------|
| **Introduced by** | Bahdanau et al., 2015 (a.k.a. *Additive Attention*) | Luong et al., 2015 (a.k.a. *Multiplicative Attention*) |
| **Computation** | Uses a feedâ€‘forward network: score = \(v^T \tanh(W_h h_t + W_s s_t)\) | Uses dot product or general form: score = \(h_t^T W s_t\) |
| **Complexity** | Slightly higher (extra parameters and nonâ€‘linear layer) | Lower (simpler dot product or linear projection) |
| **Alignment Function** | Nonâ€‘linear (MLP + tanh) â†’ more flexible | Linear (dot product or bilinear) â†’ faster |
| **Performance** | Often better for small datasets or complex alignments | Often faster and scales better to large datasets |
| **Interpretability** | Provides smoother, learned alignment weights | Simpler, but may be less expressive |
| **Training Speed** | Slower due to extra parameters | Faster due to fewer computations |
| **Memory Usage** | Higher (extra weight matrices) | Lower (leaner computation) |
| **Use Cases** | Good when fineâ€‘grained alignment is critical (e.g., machine translation with small data) | Good for largeâ€‘scale tasks (e.g., summarization, speech recognition) |

---

## Advantages & Disadvantages

### Bahdanau Attention
-  Captures complex, nonâ€‘linear relationships between encoder and decoder states.  
-  Often yields better alignment quality, especially in lowâ€‘resource settings.  
-  Slower training due to extra parameters and MLP computation.  
-  More memoryâ€‘intensive.  

### Luong Attention
-  Computationally efficient (dot product is faster).  
-  Scales well to large datasets and long sequences.  
-  Simpler to implement and integrate.  
-  Less expressive; may miss subtle alignments.  
-  Can underperform in small datasets compared to Bahdanau.  

---

  
- **Bahdanau** is preferred when accuracy and nuanced alignment matter more than speed.  
- **Luong** is preferred when efficiency and scalability are the priority.  
